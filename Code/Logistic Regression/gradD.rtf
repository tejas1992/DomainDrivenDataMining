{\rtf1\ansi\ansicpg1252\cocoartf1138\cocoasubrtf230
{\fonttbl\f0\fswiss\fcharset0 Helvetica;}
{\colortbl;\red255\green255\blue255;}
\paperw11900\paperh16840\margl1440\margr1440\vieww10800\viewh8400\viewkind0
\pard\tx566\tx1133\tx1700\tx2267\tx2834\tx3401\tx3968\tx4535\tx5102\tx5669\tx6236\tx6803\pardirnatural

\f0\fs24 \cf0 function [theta, J_history] = gradientDescent2(X1, y1, theta, alpha, num_iters)\
\
y1 = y(1:300);\
m = length(y1); % number of training examples\
J_history = zeros(num_iters, 1);\
\
for iter = 1:num_iters\
[cost, grad] = costFunction(theta, X1, y1); %costFunctionReg\
theta=theta-alpha*grad;\
J_history(iter) = cost;\
\
end\
\
\pard\tx566\tx1133\tx1700\tx2267\tx2834\tx3140\tx3401\tx3968\tx4535\tx5102\tx5669\tx6236\tx6803\pardirnatural
\cf0 end}